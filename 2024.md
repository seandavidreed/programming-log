# Year: 2024

---
### Table of Contents
[January](#112023)  

### Resources
["The end of the 'Self-Taught Programmer'"](https://youtu.be/ro4oGwMb-T4?si=5SgKq55R36pFydau) [*1/1/2024*](#112024)   

### Programs
[Quicksort and Binary Search in Python](/files/dsa/sort_search.py) *[1/8/2024](#182024)*  
[Building a Stack with Two Queues](/files/dsa/stack_from_queues.py) *[1/16/2024](#1162024)*  

---
### 1/1/2024
#### GOALS
What are my main goals for this year? What follows is my attempt to answer that:  
1. Get a new job.
    * My good friend's dad got me in contact with a guy who needs a Python developer. We had a meetup a couple of weeks ago, and I reached out to him by email afterward to confirm I am interested in joining the project. I've yet to hear back, but I'm optimistic that this could become my first legitimate development job! It would be part-time, so I'd have to continue with my current job alongside it.
2. Pursue a Masters Degree from CU Boulder
    * Since the opportunity mentioned above came my way, I'm thinking this goal needs to be pushed to springtime or later. I'm still preparing for it by taking preliminary classes on Coursera, but I don't want to try to start the degree program this January while also starting the part-time job! Nevertheless, this goal still remains a priority!
3. Hone my interests / Build something robust
    * These two goals go hand in hand. I don't want to start any old project but something that will further my goals. I watched a video that inspired me to explore frontend frameworks like *React*. [This video](https://youtu.be/ro4oGwMb-T4?si=5SgKq55R36pFydau), "The end of the 'Self-Taught Programmer'," outlines important skills to learn if one wants to tand up an application from end-to-end while developing on demand skills. He bases his recommendations on the desired qualifications written in a job posting. Here they are:
      * React development, Javascript, Redux, RESTful APIs, authorization mechanisms, continuous integration and delivery techno\logies, agile scrum development methodo\logy.
    * I'm still interested in Artificial Intelligence and Deep Learning - I'd really like to continue with the fastai program, but I think it needs to be on hold for the moment while I see how the potential job opportunity pans out.
    * I really want to get back into writing stuff in the C language again, and that probably entails embedded systems, IoT, etc. If I explore that route, I want to do it right, with laser focus. I want to start on a project straight away and use it as a platform to become familiar with all the skills and techno\logies I need.
    * I'd like to get a more intuitive understanding of how the internet works. This is partly what makes me interested in IoT since it marries two arenas I'm interested in: embedded systems and the internet in general. 
    * Finally, my main focus is to get more proficient in the most marketable skills, and that probably means Python. I already am comfortable writing in Python, but I'd like to take my skills to the next level. Hopefuly this part-time job opportunity will be the vehicle for that goal!

### 1/2/2024
#### CU Boulder A\lgorithms Module 1
>Exercise 3.1-1  
>Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic  definition of $\Theta$-notation, prove that max($f(n)$, $g(n)$) $= \Theta(f(n) + g(n))$.

$\text{let } \text{max}(f(n), g(n)) = f(n), \text{ and } f(n) = n^2,\, g(n) = n,\\ 
\text{so } \Theta(f(n) + g(n)) = \Theta(n^2 + n)$

When asymptotically evaluating a function such as $n^2 + n$, we only care about the highest-order term since it will dominate the growth of the function as $n$ increases.

$\text{therefore } \Theta(f(n) + g(n)) = \Theta(n^2 + n) = \Theta(n^2) = \Theta(f(n))\\
\text{and we know } f(n) = \Theta(f(n)),\\
\text{so } f(n) = \Theta(f(n) + g(n))$.

>Exercise 3.1-2  
>Show that for any real constants $a$ and $b$, where $b > 0$, $(n + a)^b = \Theta(n^b)$.

Let's expand $(n + a)^b$, let $c_i$ for all values of $i$ be some constant we don't care about:

$n^b + c_{b-1}n^{b-1}a^1 + c_{b-2}n^{b-2}a^2 + ... + c_2n^2a^{b-2} + c_1n^1a^{b-1} + a^b$

Since $a$ and $b$ are constants and we are generally not concerned about constants in asymptotic analysis, let's "absorb" $a$ and $b$ into $c$ where possible:

$n^b + c_{b-1}n^{b-1} + c_{b-2}n^{b-2} + ... + c_2n^2 + c_1n$

Let's remove the constants entirely now:

$n^b + n^{b-1} + n^{b-2} + ... + n^2 + n$

Here we see that the first term is the highest-order term since $b$ must be greater than itself minus any positive number. We know that the highest-order term will dominate the growth of the function, and that is the term we care about. We can remove all terms of lower orders than $n^b$. Therefore,

$(n + a)^b = \Theta(n^b)$.

>Exercise 3.1-3  
>Explain why the statement, "The running time of algorithm $A$ is at least $O(n^2)$," is meaningless.

Big-O notation describes an *asymptotically-tight* **upper bound** of a given function. As such, it describes the highest possible (worst-case) running time of the function. Therefore, the above statement says "The running time of a\lgorithm $A$ is at least its highest possible running time, which is a redundant way of saying $A$ has exactly one running time. It would be correct to say that the running time of a\lgorithm $A$ is *at most* $O(n^2)$.

>Exercise 3.1-4  
>Is $2^{n+1} = O(2^n)$? Is $2^{2n} = O(2^n)$?

To the first question, the answer is yes. We can provide a constant $K$:


$\text{let } f(n) = 2^{n+1}\\
\text{let } g(n) = 2^{n}\\
f(n) = O(Kg(n)) \iff K \geq 2$

When $K \geq 2$, $g(n) \geq f(n)$ for all $N_0 > 0$.

To the second question, the answer is no. No matter what constant $K$ we provide for $g(n)$, $f(n)$ will always eventually overtake $g(n)$ for all sufficiently large $n$, which means $g(n)$ is not an upper bound of $f(n)$.

>Exercise 3.1-5 
>Prove Theorem 3.1.

First, I'll state the theorem:

For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.

By definition, $f(n) = \Theta(g(n))$ means that $0 \leq c_1g(n) \leq f(n) \leq c_2g(n),\, \forall n > N_0 \text{ and } \forall c_1: c_1 \geq 0, \text{ and } \forall c_2: c_2 \geq 0$.

By definition, $f(n) = O(g(n))$ means that $0 \leq f(n) \leq cg(n),\, \forall n \geq N_0 \text{ and } \forall c: c \geq 0$.

By definition, $f(n) = \Omega(g(n))$ means that $0 \leq cg(n) \leq f(n),\, \forall n \geq N_0 \text{ and } \forall c:c \geq 0$.

We see that $\Theta$ is merely the combination of $O$ and $\Omega$. Therefore, if both $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$ hold, then $f(n) = \Theta(g(n))$ also holds.

>Exercise 3.1-7
>Prove that $o(g(n)) \cap \omega(g(n))$ is the empty set.

$o(g(n))$ is $g(n)$ as an upper bound (not *asymptotically tight*) to some other function (suppose $f(n)$) or set of functions. That is, $0 \leq f(n) < g(n)$. 

$\omega(g(n))$ is $g(n)$ as a lower bound (not *asymptotically tight*) to some other function or set of functions. That is $0 \leq g(n) < f(n)$.

Notice that in $f(n) = o(g(n))$, $f(n) \neq g(n)$. Additionally, in $f(n) = \omega(g(n))$, $f(n) \neq g(n)$. Therefore, $o(g(n)) \cap \omega(g(n)) = \emptyset$.

>Exercise 3.1-8
>We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n,m)$, we denote by $O(g(n,m)) the set of functions

>$O(g(n,m)) = {f(n,m): \exists(c, n_0, m_0): 0 \leq f(n,m) \leq cg(n,m)\ \forall n \geq n_0 \text{ or } m \geq m_0}$

$\Omega(g(n,m)) = {f(n,m): \exists(c, n_0, m_0): 0 \leq cg(n,m) \leq f(n,m)\ \forall n \geq n_0 \text{ or } m \geq m_0}$.

$\Theta(g(n,m)) = {f(n,m): \exists(c_1, c_2, n_0, m_0): 0 \leq c_1g(n,m) \leq f(n,m) \leq c_2g(n,m)\ \forall n \geq n_0 \text{ or } m \geq m_0}$.

### 1/3/2024
#### CU Boulder A\lgorithms Module 1
>Exercise 3.2-1  
>Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n) + g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n) \cdot g(n)$ is monotonically increasing.

For a function to be monotonically increasing, $m \leq n \iff f(m) \leq f(n)$ must hold. If this rule holds for both $f(n)$ and $g(n)$, then it follows that $m_f + m_g \leq n_f + n_g$, where the subscript identifies the function to which the variable is input. Let's prove it:

$\text{let } m_f = 1,\ n_f = 2,\ m_g = 3,\ n_g = 4$  
$\text{since }1 \leq 2 \text{ and } 3 \leq 4$.  
$\text{combined } 1 + 3 \leq 2 + 4 \text{ the inequality is still true}$.

We can see that this will hold for all values of $m$ and $n$. We have an initial inequality $m_f \leq n_f$ and the value $m_g$ we add to $m_f$ by definition will be less than or equal to the value $n_g$ we add to $n_f$, otherwise $m_g \leq n_g$ is not true. We can prove that $f(n) \cdot g(n)$ is monotonically increasing by the same \logic with the additional constraint that $f(n)$ and $g(n)$ are nonnegative.

Given that $f(n)$ and $g(n)$ are monotonically increasing functions, let's prove $m \leq n \iff f(g(m)) \leq f(g(n))$.

$\text{We know that } m \leq n \iff g(m) \leq g(n) \text{ holds}$.  
$\text{We know that } m \leq n \iff f(m) \leq f(n) \text{ holds}$.  
$\text{In suppling } g(m) \text{ and } g(n) \text{ as inputs to } f,$  
$\text{we see that } g(m) \leq g(n) \text{ bear the same relation as } m \leq n,$  
$\text{so } g(m) \leq g(n) \iff f(g(m)) \leq f(g(n))$.

>Exercise 3.2-2  
>Prove equation (3.16) -> $a^{\log_b c} = c^{\log_b a}$
 
$a^{\log_b c} = c^{\log_b a}$  

Take $\log_a$ of both sides.  
$\log_a(a^{\log_b c}) = \log_a(c^{\log_b a})$  

Follow *\log of a power* rule.  
$\log_b c \cdot \log_a a = \log_b a \cdot \log_a c$  

Simplify.  
$\log_b c = \log_b a \cdot \log_a c$  

Rewrite.  
$\frac{\log_b c}{\log_b a} = \log_a c$ 

Follow change of base rule and we're done.  
$\log_a c = \log_a c$


>Exercise 3.2-5  
>Which is asymptotically larger: $\lg(\lg^*n)$ or $\lg^*(\lg\ n)$?

I got some help for this problem, and now I can confidently say I understand it!

First, I'll write the *recurrent equation* for iterated \log:

$
\lg^*n = \begin{cases}
            0 & \text{if } n \leq 1 \\
            1 + \lg^*(\lg\ n) & \text{if } n > 1
        \end{cases}
$

We'll use a limit to compare these two functions:

$\displaystyle\lim_{x \rightarrow \infty} \frac{\lg(\lg^*n)}{\lg^*(\lg\ n)}$

Let's supply $2^n$ to $n$.

$\displaystyle\lim_{x \rightarrow \infty} \frac{\lg(\lg^*2^n)}{\lg^*(\lg\ 2^n)}$

We find the following when we plug $2^n$ into the *recurrent equation*.  

$\lg^*2^n = 1 + \lg^*n$

We also know,

$\lg\ 2^n = n$

Putting it together, we get

$\displaystyle\lim_{x \rightarrow \infty} \frac{\lg(1+\lg^*n)}{\lg^*n}$

We can simplify $lg^*n$ to $n$ for both functions since it won't change how they relate to one another.

$\displaystyle\lim_{x \rightarrow \infty} \frac{\lg(1+n)}{n}$

Here we can see that as $n \rightarrow \infty$, the limit goes to $0$ since the denominator grows faster than the numerator. Therefore,

$\lg^*(\lg\ n) = \Omega(\lg(\lg^*n))$ and $\lg(\lg^*n) = O(\lg^*(lg\ n))$

### 1/5/2024
#### CU Boulder Algorithms Module 1
>Problem 3-1  
Let  
$p(n) = \displaystyle\sum_{i=0}^{d} a_i n^i$  
where $a_d > 0$, be a degree-$d$ polynomial in $n$, and let $k$ be a constant. Use the definitions of the asymptotic notations to prove the following properties.  
a. If $k \geq d$, then $p(n) = O(n^k)$.  
b. If $k \leq d$, then $p(n) = \Omega(n^k)$.  
c. If $k = d$, then $p(n) = \Theta(n^k)$.  
d. If $k > d$, then $p(n) = o(n^k)$.  
e. If $k < d$, then $p(n) = \omega(n^k)$.  

a. In a degree-$d$ polynomial, the highest-order term is $n^d$. In measuring asymptotic growth, we only care about the highest-order term, so for $n^k$ where $k \geq d$, $n^d = O(n^k)$.  
b. The logic in answer *a* holds here as well but with $n^k$ as a lower bound.  
c. $p(n) = \Theta(n^k)$ means $\exists c_1, c_2: 0 \leq c_1n^k \leq p(n) \leq c_2n^k,\ \forall n \geq n_0$. The highest-order term is what drives the growth of a polynomial. Suppose $p(n) = n^2 + 3n + 5$. We can think of $n^k$ as a degree-$k$ polynomial of the form $q(n) = n^k + 0n^{k - 1} + ... + 0$. Even though the lower-order terms are smaller than those of $p(n)$, provided a sufficiently small constant $c_1$ and a sufficiently large constant $c_2$, we can, respectively, cause $q(n)$ to be less than $p(n)$ and greater than $p(n)$ for all values beyond an overtake point $n_0$.  
d. With $p(n) = o(n^k)$, we are saying $p(n) \in o(n^k)$. Little $o$ is an upper bound that is not asymptotically tight. For example, a linear function, $n$, is well below the polynomial upper bound, $n^2$, but it is indeed bounded by it. For all $n \geq n_0$, as $n$ grows, the output of the linear function will far fall below that of $n^2$ so that they aren't growing at the same rate. This is true whenever $k > d$.  
e. We use the same logic as in answer *d*, but with $n^k$ as a lower bound.

### 1/8/2024
#### CU Boulder Algorithms Module 1 Exercises
>Problem 3-3  

In my own words, this problem presented me with 30 functions and tasked me with organizing them like so:

$g_{30} = \Omega(g_{29}),\ g_{29} = \Omega(g_{28}),\ ...\ ,\ g_2 = \Omega(g_1)$.

I was awfully close in my answer, but what I've provided here are all the correct answers, which I verified with the solutions manual. I have them here for my own reference:

$$
    2^{2^{n+1}},\ 2^{2^n}, (n+1)!,\ n!,\ n2^n,\ e^n,\newline 2^n,\ (\frac{3}{2})^n,\ (\lg n)!,\ (\lg n)^{\lg n},\ n^{\lg(\lg n)},\ n^3,\newline n^2,\ 4^{\lg n},\ \lg(n!),\ n\lg n,\ 2^{\lg n},\ n,\newline \sqrt{2}^{\lg n},\ 2^{\sqrt{2\lg n}},\ \lg^2n,\ \ln n,\ \sqrt{\lg n},\ \ln(\ln n),\newline 2^{\lg^*n},\ \lg^*n,\ \lg^*(\lg n),\ \lg(\lg^*n),\ n^{1/\lg n},\ 1
$$

There were a few interesting equivalences of which I took note while solving this problem. Here they are below:

$n^2 = 4^{\lg n}$  
$n^{\lg(\lg n)} = (\lg n)^{\lg n}$  
$n = 2^{\lg n}$  
$n^{1/\lg n} = 2$  

I also uncovered an interesting relationship, which I try my best to describe as follows:

We have a linear function $f(n) = n$.  
We have another function $g(n) = c^{\log_b n}$, where $\exists c: c > 0,\ c \in \N$ and $\exists b: b > 0,\ b \in \N$.   
$g(n) > f(n),\ \forall c > b,\ \forall n > N_0$.  
$g(n) = f(n),\ \forall n$.  
$g(n) < f(n), \forall c < b,\ \forall n > N_0$.

#### CU Boulder Algorithms Module 1 - Binary Search
Since I'm already well familiar with Binary Search, I decided to go beyond the material in the lecture and spin up my own implementation in Python, just to practice. Of course, for Binary Search to work, the array to be searched must be sorted, so I wrote a program that first sorts the array with Quicksort and then offers the user the option to search using Binary Search. 

I got the idea to try implementing Quicksort from a book I'm currently reading called *Algorithms*; it's one from *The MIT Press Essential Knowledge Series*. It's really an birds-eye view of the topic of algorithms, but I've found it helpful for organizing my knowledge on the subject. In chapter 4, it describes the function of Quicksort really well without using any pseudocode or math. I took that as a challenge to transform it from a concept to a program. It worked. As simple as this was in the grand scheme, it was still a thrill. I love working with algorithms. Below, I've included my program.

```
import random
import sys
import os

def quick_sort(nums: list) -> list:
    # Base case. Nothing to do here since lists
    # of size 1 are already sorted (trivially).
    if len(nums) < 2:
        return nums

    # Recursive case. Pick random element and
    # pivot all other elements around it.
    rand_idx = random.randint(0, len(nums) - 1)
    aux = [0] * len(nums)

    i, j, k = -1, 0, len(nums) - 1
    
    for i in range(len(nums)):
        if i == rand_idx:
            continue
        if nums[i] <= nums[rand_idx]:
            aux[j] = nums[i]
            j = j + 1
        else:
            aux[k] = nums[i]
            k = k - 1

    aux[j] = nums[rand_idx]

    nums = list(aux)
    del aux
    nums[:j] = quick_sort(nums[:j])
    nums[j+1:] = quick_sort(nums[j+1:])

    return nums

def binary_search(nums: list, a: int, b: int, find: int) -> int:
    if a > b:
        return -1

    mid = a + (b - a) // 2

    if find < nums[mid]:
        return binary_search(nums, a, mid-1, find)
    elif find > nums[mid]:
        return binary_search(nums, mid+1, b, find)
    else:
        return mid

def main():
    if len(sys.argv) != 2:
        print("Usage: python3 sort_search.py <INTEGER>")
        return -1

    try:
        find = int(sys.argv[1])
    except ValueError:
        print("Usage: python3 sort_search.py <INTEGER>")
        return -1

    nums = []
    for i in range(10):
        nums.append(random.randint(0, 20))
    new_nums = quick_sort(nums)
    print(new_nums)

    while True:
        print(f"Searching for {find}")
        result = binary_search(new_nums, 0, len(new_nums)-1, find)
        
        if result == -1:
            print("Not Found")
        else:
            print(f"{find} found at index {result}")

        choice = input(f"Do you want to search again? [y/n]: ")

        if choice.upper() != 'Y':
            return
        while True:
            find = input("Enter new integer: ")
            try:
                find = int(find)
                break
            except ValueError:
                print("Please provide an integer.")

if __name__ == "__main__":
    main()
seandavidreed ~/my_programs/algorithms_practice $
```

I had fun writing this; however, I do need to look up examples of Quicksort, on GeeksForGeeks for example, to see if there are any profound differences in implementation or if there are any optimizations to be made. My implementation does not sort the list in-place, as I would prefer it to do, so I might look into that more.

### 1/9/2024
#### Random Discovery
I was lying in bed unable to sleep last night because I kept thinking about algorithms and mathematics. Once my brain gets started on those topics it's hard to stop it! I was thinking about how logarithms are merely recursive division by the same divisor. This conception has helped me tremendously in making the damn things more intuitive. Further, I used the same form to understand the iterated log, which is recursively taking the log of a number. 

Then I thought what is the analog to all this with multiplication? It was a question to which I already knew the answer: exponentiation is recursively multiplying with the same multiplier. Logarithmic and exponential functions are inverse to one another. Finally, I asked, what is inverse to iterated log? The answer hit me almost immediately! I thought back to a YouTube video I watched about an abstruse mathematical artifact called *Tetration*. [This is the video](https://www.youtube.com/watch?v=Ea1_1aVfwl4). *Tetration* is the inverse of the iterated log! Here's a precise formulation:

$f(n)=\displaystyle{}^n2 \land g(n) = \lg^*n: \forall n > 0,\ f(n) = g^{-1}(n) \land g(n) = f^{-1}(n)$

Suppose I calculate $f(4) = \displaystyle{}^4{2}$. 

$\displaystyle{}^4{2} = \underbrace{2^{2^{2^2}}}_{n=4} = 65536$. We find that $f(4) = 65536$.  

Let's provide that output as input to $g(n)$, so $g(65536) = \lg^*65536$.  
The iterated log function is defined using a recurrent equation: 

$\lg^*(n) := 
    \begin{cases}
        0 & \text{if }n \leq 1\\
        1 + \lg^*(\lg n) & \text{if }n > 1
    \end{cases}$

$\lg^*(65536) = 16 \rightarrow \lg^*(16) = 4 \rightarrow \lg^*(4) = 2 \rightarrow \lg^*(2) = 1$

The function recursed $4$ times, so $g(65536) = 4$. Hence, $f(n)$ and $g(n)$ are inverses of each other!

I extended this concept of recursion throughout all basic arithmetical operations:

Division:

$divide(a,\ b) :=
    \begin{cases}
        0 & if\ a = 0\\
        1 + divide(a - b,\ b) & if\ a \geq 1
    \end{cases}$

Multiplication:

$multiply(a,\ b) := 
    \begin{cases}
        0 & if\ b = 0\\
        a + multiply(a,\ b - 1) & if\ b \geq 1
    \end{cases}$

Logarithm:

$log(b,\ n) :=
    \begin{cases}
        1 & if\ n \leq 1\\
        n \div log(b,\ n - b) & if\ n > 1
    \end{cases}$

Exponentiation:

$power(b,\ p) :=
    \begin{cases}
        1 & if\ p = 0\\
        b \times power(b,\ p-1) & if\ p \geq 1
    \end{cases}$

Iterated Logarithm:

$\lg^*(n) := 
    \begin{cases}
        0 & \text{if }n \leq 1\\
        1 + \lg^*(\lg n) & \text{if }n > 1
    \end{cases}$

Tetration:

$tetrate(t,\ b) := 
    \begin{cases}
        0 & if\ t = 0\\
        b^{tetrate(t-1,\ b)} & if\ t > 1
    \end{cases}$

#### CU Boulder Algorithms Module 1 - Binary Search
I tried to create a pivot algorithm for Quicksort that modifies the list in place. While I succeeded, the algorithm seems to be grossly inefficient, which I sort of expected given the way I designed it. I'll probably try to improve it later, but for now, here's what I came up with: [pivot.py](/files/dsa/pivot.py).

### 1/15/2024
#### CU Boulder Algorithms Module 1
I completed Module 1 today. I'd been working on it intermittently the past few days. I hope to move more quickly through module 2 - I'll probably do more skimming in the CLRS book to accomplish this (of course, I won't skim where I don't understand). Here's a few notes from the final optional lecture:

1. $1.5n^e + 2.2n\lg n + 3n \rightarrow \Theta(n^2)$
2. $f = 2^n,\ g = 2^{2n},\ f \neq \Theta(g),\ g = f^2$
3. $f = 2^n,\ g = 3^n,\ f \neq \Theta(g)$. Let's rewrite $3^n$ as $2^{(\log_2 3)n}$, so we can see that this is the same as example 2.
4. We have the rule $(\log_a b)(\log_b c) = \log_a c$. Suppose $f = \log_2 n$ and $g = \log_5 n$. We can rewrite $g \rightarrow g = \underbrace{\log_5 2}_{constant} \cdot \underbrace{\log_2 n}_{f}$. Therefore $f = \Theta(g)$.

Miscellaneous:
1. $2^{(\log n)^2}$ is an example of *poly-log time*. 
2. $\underbrace{2^{2^{2^{2^{...}}}}}_{n\ times}$ is an example running time for *non-elementary algorithms*. (This seems yet another example of *tetration*!)

### 1/16/2024
#### CU Boulder Algorithms Module 2 - Answering CLRS Questions
>Exercise 10.1-2  
>Explain how to implement two stacks in one array $A[1...n]$ in such a way that neither stack overflows unless the total number of elements in both stacks together is $n$. The $\text{Push}$ and $\text{Pop}$ operations should run in $O(1)$ time.

Stack 1 ($S1$) will grow with increasing index values starting from index 1 and Stack 2 ($S2$) will grow with decreasing index values starting from index $n$. The data structure $A$ will have two attributes, $S1.top$ and $S2.top$. The $\text{Push}$ method will have two parameters, `PUSH(stack, element)`; I don't include $A$ in the parameters since I'm conceiving of $A$ as a class, of which `PUSH()` is a method. The programmer specifies which stack on which to push, `PUSH(S2, -6)`, and the method will check against overflow in the following way:

```
PUSH(stack, element):
    if ABSOLUTE_VALUE(s1.top - s2.top) == 1:
        error "overflow"
    if stack == s1:
        s1.top += 1
        A[s1.top] = element
    else:
        s2.top += 1
        A[s2.top] = element
```

>Exercise 10.1-4  
>Rewrite $\text{Enqueue}$ and $\text{Dequeue}$ to detect underflow and overflow of a queue.

```
ENQUEUE(Q, x):
    if Q.head == (Q.tail % Q.length) + 1:
        error "overflow"
    
    Q[Q.tail] = x
    if Q.tail == Q.length:
        Q.tail = 1
    else:
        Q.head = Q.head + 1

DEQUEUE(Q):
    if Q.head == (Q.tail % Q.length) + 1:
        error "underflow"

    x = Q[Q.head]
    if Q.head == Q.length:
        Q.head = 1
    else:
        Q.head = Q.head + 1

    return x
```

>Exercise 10.1-5  
>Whereas a stack allows insertion and deletion of elements at only one end, and a queue allows insertion at one end and deletion at the other end, a ***deque*** (double-ended queue) allows insertion and deletion at both ends. Write four $O(1)$-time procedures to insert elements into and delete elements from both ends of a deque implemented by an array.

```
ENQUEUE-TAIL(Q, x):
    if Q.head == (Q.tail % Q.length) + 1
        error "overflow"
    
    Q[Q.tail] = x
    Q.tail = (Q.tail == Q.length) ? 1 : Q.tail + 1

ENQUEUE-HEAD(Q, x):
    if Q.head == (Q.tail % Q.length) + 1
        error "overflow"
    
    Q.head = (Q.head == 1) ? Q.length : Q.head - 1
    Q[Q.head] = x

DEQUEUE-TAIL(Q):
    if Q.head == Q.tail:
        error "underflow"
    
    Q.tail = (Q.tail == 1) ? Q.length : Q.tail - 1
    return Q[Q.tail]

DEQUEUE-HEAD(Q):
    if Q.head == Q.tail:
        error "underflow"

    x = Q[Q.head]
    Q.head = (Q.head == Q.length) ? 1 : Q.head + 1

    return x
```

>Exercise 10.1-6  
>Show how to implement a queue using two stacks. Analyze the running time of the queue operations.

To implement a queue with two stacks, we will call one stack our "enqueueing" stack and the other our "dequeueing" stack. When `ENQUEUE()` gets called, it first checks whether the *dequeueing* stack is empty. If it is, the new element is simply pushed to the *enqueueing* stack, provided there's no overflow. However, if the *dequeueing* stack is not empty, the method first pops and pushes each element one at a time from the *dequeueing* stack to the *enqueueing* stack. Then, it pushes the new element to the *enqueueing* stack. This whole process is exactly inverted when `DEQUEUE()` is called.

This implementation will run somewhat efficiently if enqueueing and dequeueing tend to happen in batches. However, if they alternate frequently, the queue will be less efficient. Therefore, the worst case for a single `ENQUEUE()` or `DEQUEUE()` call is $O(n)$ since all elements must be transferred from one stack to the other. The best case is $\Omega(1)$.

>Exercise 10.1-7  
>Show how to implement a stack using two queues. Analyze the running time of the stack operations.

I had some fun answering this question. The obvious answer was to follow these rules:

1. To push onto the "stack," always enqueue a new element to whichever queue is non-empty. If they are both empty, either is fine.
2. To pop from the "stack," dequeue $n - 1$ elements from the non-empty queue and enqueue them in the other queue one at a time, where $n$ is the total number of elements in the "stack." Finally, dequeue and return the $nth$ element.

Every `push()` will take $\Theta(1)$ time. Every `pop()` will take $\Theta(n)$ time. This isn't terrible, but I wanted to see if I could reduce the running time of the `pop()` operation. I tried to devise a strategy where the `push()` operation alternates enqueueing the two queues and the `pop()` operation alternates dequeueing the two queues. I had a fairly structured idea for how to keep track of the order of insertion so as to make sure the right element was always popped. However, once I coded the thing, I realized there was not really an advantage to my design.

Suppose we instantiate a new stack with this design and push several elements to it. The very first `pop()` operation would take precisely $\Theta(\lfloor n/2 \rfloor)$ time. Each subsequent `pop()` would take the ordinary $\Theta(n)$ time. I suppose there would be a marginal benefit if the stack were used in such a way where pushes and pops frequently alternated - in that specific circumstances, `pop()` could run with $\Theta(1)$ time. All in all, the extra complexity in my design produced optimizations only for very particular situations. Probably not worth it!

I decided to go ahead and implement the simplest idea in Python. Here is the program: [stack_from_queues.py](/files/dsa/stack_from_queues.py).

### 1/21/2024
#### CU Boulder Algorithms Module 2 - Exercises
>Exercise 6.1-1  
>What are the minimum and maximum numbers of elements in a heap of height $h$?  

The maximum is $2^h$ and the minimum is $2^{h + 1} - 1$.  

>Exercise 6.1-2  
>Show that an $n$-element heap has height $\lfloor \lg n \rfloor$.

Every heap has a single node for a root and for each complete level of the tree, the number of nodes is double that of the previous level. Therefore, we may demonstrate that, for a given height $h$, the number of nodes in the tree is $n \leq \displaystyle \sum_{i = 0}^{h} 2^h$. 

For a tree of height $0$, we have $2^0 = 1$, this is clearly true since a tree of 1 node is without edges and therefore has no height. We can show in this case that $h = \lg n$ is true: $0 = \lg 1$. For a tree of height 1, we have $2^0 + 2^1 = 3$, which s the maximum number of nodes for a tree of height $1$. We can see that $1 = \lfloor \lg 3 \rfloor$ is true. In this case, if the number of nodes exceeds $3$, an additional edge must be added, increasing the height of the tree since $2 = \lg 4$. The next number of nodes for which we must increase the height is 8 since $3 = \lg 8$ and $2 = \lfloor \lg 7 \rfloor$. All together, we see that the rule holds $h = \lfloor \lg n \rfloor$.

>Exercise 6.1-3  
>Show that in any subtree of a max-heap, the root of the subtree contains the largest value occurring anywhere in that subtree.  

Given the max-heap property, $A[\text{parent}(i)] \geq A[i]$, we know that the child nodes of any given parent node will be smaller than the parent. Therefore, it's simple to see that if we choose any parent node as a point of division from the rest of the tree, it will be the root and largest node of the resulting subtree.

>Exercise 6.1-4  
>Where in a max-heap might the smallest element reside, assuming that all elements are distinct?

In a max-heap, the smallest element would reside anywhere in the bottom layer of leaf nodes.

>Exercise 6.1-5  
>Is an array that is in sorted order a min-heap?

Yes, and a simple experiment can prove it. Take the index of any element $p$ that has child nodes; by definition of a min-heap, its child nodes $l$ and $r$ must both be greater than or equal to their parent, $p \leq l \land p \leq r$. The sorted array satisfies this condition. An array in ascending order is always a min-heap.

It is worth noting that the consequent is not also true: a min-heap is not necessarily a sorted array. $l$ comes before $r$ in the min-heap array, yet $l \leq r$ is _not_ a required condition, yet it _is_ required for a sorted array. A min-heap is not always a sorted array.

>Exercise 6.1-6  
>Is the array with values $\langle 23, 17, 14, 6, 13, 10, 1, 5, 7, 12 \rangle$ a max-heap?

It is not. The 4th element, 6, has as its child nodes, 5 and 7. 5 satisfies the max-heap property, but 7 does not.

>Exercise 6.1-7  
>Show that, with the array representation for storing an $n$-element heap, the leaves are the nodes indexed by $\lfloor n/2 \rfloor + 1, \lfloor n/2 \rfloor + 2, ..., n$.  

Let's have an example min-heap of size $n = 7$: $\langle 2, 4, 3, 6, 6, 4, 5 \rangle$. The leaf nodes are $\langle 6, 6, 4, 5 \rangle$ and their indices are 4, 5, 6, and 7. Let's perform the calculations:

$\lfloor 7/2 \rfloor + 1 = 4$  
$\lfloor 7/2 \rfloor + 2 = 5$  
$\lfloor 7/2 \rfloor + 3 = 6$  
$\lfloor 7/2 \rfloor + 4 = 7$

This makes sense because a heap is organized as a binary tree, the completed levels of which double the size of their previous levels. We can therefore expect the final level of the heap, that of the leaf nodes, to begin at exactly halfway through the array.

### 1/22/2024
#### CU Boulder Algorithms Module 2 - Exercises
>Exercise 6.2-2  
>Starting with the procedure MAX-HEAPIFY, write pseudo code for the procedure MIN-HEAPIFY(A, i), which performs the corresponding manipulation on a min-heap. How does the running time of MIN-HEAPIFY compare to that of MAX-HEAPIFY?

```
MIN-HEAPIFY(A, i):
    l = LEFT(i)
    r = RIGHT(i)
    
    if A[l] < A[i]:
        smallest = l
    else:
        smallest = A[i]

    if A[r] < smallest:
        smallest = r

    if smallest != i:
        SWAP(A, i, smallest)
        MIN-HEAPIFY(A, smallest)   
```

The running time of MIN-HEAPIFY is identical to that of MAX-HEAPIFY at $O(\lg n)$.

>Exercise 6.2-3  
>What is the effect of calling MAX-HEAPIFY(A, i) when the element A[i] is larger than its children?

The function will take six steps, evaluating each condition as false, and exit without making any changes to $A$.

>Exercise 6.2-4  
>What is the effect of calling MAX-HEAPIFY(A, i) for $i > A.heap-size/2$?

This is effectively called MAX-HEAPIFY on a leaf node, which by definition has no child nodes. Therefore, there is nothing to be done, and the algorithm should have a function to check for this case before making the $l = \text{LEFT}(i) \text{ and } r = \text{RIGHT}(i)$ assignments.

>Exercise 6.2-5  
>The code for MAX-HEAPIFY is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, which might cause some compilers to produce inefficient code. Write an efficient MAX-HEAPIFY that uses an iterative control construct (a loop) instead of recursion.

```
MAX-HEAPIFY(A, i):
    while True:
        if i >= A.heap-size // 2:
            break
    
        if A[LEFT(i)] > A[i]:
            largest = LEFT(i)
        else:
            largest = i

        if A[RIGHT(i)] > largest:
            largest = A[RIGHT(i)]

        if largest == i:
            break

        SWAP(A, i, largest)
        i = largest    
```

### 1/23/2024
#### CU Boulder Algorithms Module 2 - Exercises
>Exercise 6.3-2  
>Why do we want the loop index $i$ in line 2 of `BUILD-MAX-HEAP` to decrease from $\lfloor A.length / 2 \rfloor$ to 1 rather than increase from 1 to $\lfloor A.length / 2 \rfloor$?

We know that all indices greater than $\lfloor A.length / 2 \rfloor$ are those of leaf nodes, and as such, they are trivially max heaps. Therefore, we loop descending from $\lfloor A.length / 2 \rfloor$ so as not to touch these leaf nodes. If `BUILD-HEAP-MAX` were to operate the other way, then we would have to perform checking operations for every leaf node. In fact, this extra checking would occur all the way down the heap.

>Exercise 6.3-3  
>Show that there are at most $\lceil n/2^{h+1} \rceil$ nodes of height $h$ in any $n$-element heap.

The height of a node is the number of edges of the longest simple path from it to a leaf node in its sub-heap. In a heap, each level with no leaf nodes is double that of the previous level. 

A heap of size $n=1$ has a height of $h=0$. As soon as we increase it to $n=2$, a power of 2, a new level is created, thereby increasing the height of the tree to $h=1$.

A heap of size $n=3$ has a height of $h=1$ and is a perfect binary tree, meaning all parents have exactly two children. As soon as we increase it to $n=4$, a power of 2, a new level is created, thereby increasing the height of the tree to $h=2$.

One more example.

A heap of size $n=7$ has a height of $h=2$ and is a perfect binary tree. As soon as we increase it to $n=8$, a power of 2, a new level is created, thereby increasing the height of the tree to $h=3$.

The height of a heap always increases when we reach a power of 2. This means that $n/2^{h+1}$ will always be bounded thus:

$1/2 \leq n/2^{h+1} < 1$  
Therefore, it will always evaluate $\lceil n/2^{h+1} \rceil = 1$.

This is what we expect since only the root node of a heap has a height of $h$, which is the height of the tree.

### 1/24/2024
#### CU Boulder Algorithms - Module 2
>Exercise 6.4-2  
>Argue the correctness of `HEAPSORT` using the following loop invariant:  

>At the start of each iteration of the **for** loop of lines 2-5, the subarray $A[1...i]$ is a max-heap containing the $i$ smallest elements of $A[1...n]$, and the subarray $A[i+1...n]$ contains the $n-i$ largest elements of $A[1...n]$, sorted.

Here's the pseudocode from the book as a reference:  
```
1   HEAPSORT(A):
2       BUILD-MAX-HEAP(A)
3       for i = A.length down to 2:
4           exchange A[1] with A[i]
5           A.heap-size = A.heap-size - 1
6           MAX-HEAPIFY(A, 1)
```

Initialization: Before the first iteration of the for loop, `A[1...n]` is a max-heap and trivially the non-existent subarray `A[i+1...n]`, the last $n$ being exclusive, is sorted in ascending order, containing the largest elements of array $A$.

Maintenance: After the first iteration, `A[1...i]` is still a max-heap because of `MAX-HEAPIFY` and now `A[i+1...n]` is, still trivially, a sorted array in ascending order of size 1; moreover, it contains the largest elements of array $A$ since it always receives the root of the max-heap, which will always be the largest element after the call to `max-heapify`. After the next iteration, we still have a max-heap in `A[1...i]` and now `A[i+1...n]` is of size 2 and non-trivially sorted in ascending order.

Termination: The loop terminates when $i < 2$, at which point our sorted array is now the size of the original array.

>Exercise 6.4-3  
>What is the running time of `HEAPSORT` on an array $A$ of length $n$ that is already sorted in increasing order? What about decreasing order?

When the array is already in increasing order, `HEAPSORT` will run with the usual $O(n \lg n)$ since to build the max-heap, we will have to rearrange most of the elements. When the array is in decreasing order, `BUILD-MAX-HEAP` will run in $\Theta(\lg n)$ instead of linear time since it will merely step up each level of the tree without calling `BUBBLE-DOWN` or `BUBBLE-UP`. However, this won't be enough to change the overall running time of `HEAPSORT` from $\Theta(n \lg n)$.

>Exercise 6.4-5  
>Show that when all elements are distinct, the best-case running time of `HEAPSORT` is $\Theta(n \lg n)$.

The first part of `HEAPSORT`, `BUILD-MAX-HEAP`, runs in $\Theta(n)$ time, which means it is lower bounded, $\Omega(n)$. This running time will not change no matter what the state of the input array.

The second part of the `HEAPSORT` algorithm is given below:

```
for i = A.length down to 2:         # c1 * (n / 2)
    swap(A[1], A[i])                # c2
    A.heap-size = A.heap-size - 1   # c3
    MAX-HEAPIFY(A, 1)               # C4 * lg n
```

This second part runs in $\Theta(n \lg n)$ - its running time is lower bounded by $\Omega(n \lg n)$. In other words, the running time in the best case is the same as in $\Theta$ since the max-heap must still be unpacked into a sorted array regardless of whether is was already sorted to begin with.


### 1/26/2024
#### CU Boulder Algorithms Module 2 - 6.4 Exercises
>Exercise 6.5-3  
>Write pseudocode for the procedures `HEAP-MINIMUM`, `HEAP-EXTRACT-MIN`, `HEAP-DECREASE-KEY`, and `MIN-HEAP-INSERT` that implement a min-priority queue with a min-heap.

```
HEAP-MINIMUM(A):
    return A[1]

HEAP-EXTRACT-MIN(A):
    if A.heap-size < 1:
        error "underflow"

    min = HEAP-MINIMUM(A)
    A[1] = A[A.heap-size]
    A.heap-size = A.heap-size - 1
    MIN-HEAPIFY(A, 1)

HEAP-DECREASE-KEY(A, i, key):
    if key > A[i]:
        error "new key is greater than current key"

    A[i] = key

    while i > 1 and A[i] < A[PARENT(i)]:
        SWAP(A, i, PARENT(i))
        i = PARENT(i)
    
MIN-HEAP-INSERT(A, key):
    A.heap-size = A.heap-size + 1
    A[A.heap-size] = inf
    HEAP-DECREASE-KEY(A, A.heap-size, key)
```

>Exercise 6.5-4  
>Why do we bother setting the key of the inserted node to $-\infty$ in line 2 of `MAX-HEAP-INSERT` when the next thing we do is increase its key to the desired value?

When we increase `heap-size`, we add another index to our heap, within the scope of `A.length`. If this index has never been used before, it will contain a garbage value; if it has been used, it likely contains a previously deleted value that is unknown to us. Therefore, we set it to $-\infty$ to make sure that `HEAP-INCREASE-KEY` works properly; otherwise, it might throw the error, "new key is smaller than current key."

>Exercise 6.5-5  
>Argue the correctness of `HEAP-INCREASE-KEY` using the following loop invariant:

>At the start of each iteration of the **while** loop of lines 4-6, the subarray $A[1...A.\text{heap-size}]$ satisfies the max-heap property, except that there may be one violation: $A[i]$ may be larger than $A[PARENT(i)]$.

>You may assume that the subarray $A[1...A.\text{heap-size}]$ satisfies the max-heap property at the time `HEAP-INCREASE-KEY` is called.

Initialization: at the start of the first iteration, we have a max-heap with one element $k$ out of place. We check to see if $k$ is greater than its parent and exchange them if it is.

Maintenance: if the loop begins again, we know we still have a max-heap with element $k$ out of place. By definition of the max-heap, when we swapped $k$ with its parent in the first iteration, we gave $k$ a new parent that is closer to $k$ in value than the previous parent, or else we moved it to the root. So with every iteration of the loop where $k < parent(k)$, the distance between $k$ and its parent shrinks.

Termination: when $k \geq parent(k)$, the loop terminates. Each iteration of the loop brings us closer to this result or else places the element at the root, which also terminates the loop.

>Exercise 6.5-6  
>Each exchange operation on line 5 of `HEAP-INCREASE-KEY` typically requires three assignments. Show how to use the idea of the inner loop of `INSERTION-SORT` to reduce the three assignments down to just one assignment.

For reference, I'll write pseudocode for `INSERTION-SORT`:

```
INSERTION-SORT(A):
    for i = 1 to A.length - 1:
        j = i + 1
        key = A[j]
        while A[j - 1] > key and j > 1:
            A[j] = A[j - 1]
            j = j - 1
        A[j] = key
```

In the above pseudocode, we're holding on to $key$ while we shift all larger elements to the right; then we insert it into the resulting empty space. This is more efficient than constantly swapping $key$ with each element.

```
HEAP-INCREASE-KEY(A, i, key):
    if key < i:
        error "key is smaller than current key"

    while key > A[PARENT(i)] and i > 1:
        i = PARENT(i)

    A[i] = key
```

>Exercise 6.5-8  
>The operation `HEAP-DELETE(A, i)` deletes the item in node $i$ from heap $A$. Give an implementation of `HEAP-DELETE` that runs in $O(\lg n)$ time for an $n$-element max-heap.

```
HEAP-DELETE(A, i):
    A[i] = A[A.heap-size]
    A.heap-size = A.heap-size - 1

    while True:
        l = LEFT(i)
        r = RIGHT(i)

        largest = i
        if A[l] > largest:
            largest = l
        
        if A[r] > largest
            largest = r

        if largest == i:
            break
```

>Exercise 6.5-9  
>Give an $O(n \lg k)$-time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. (Hint: use a min-heap for $k$-way merging). 

To answer this question, I wrote a program in C++. [Here it is](/files/dsa/k_way_merge/k_way_merge.cpp).

### 1/29/2024
#### CU Boulder Algorithms - Module 2
>Exercise 11.1-2  
>A bit vector is simply an array of bits (0s and 1s). A bit vector of length $m$ takes much less space than an array of $m$ pointers. Describe how to use a bit vector to represent a dynamic set of distinct elements with no satellite data. Dictionary operations should run in $O(1)$ time.

Since there is to be no satellite data in this implementation, the keys themselves are the data, and we use them to index into the table, which simply stores a $1$ is the data is present in and a $0$ if the data is absent from the table.

#### CU Boulder Algorithms - Module 2 Programming Assignment
Over the course of the day, I completed each task in the final assignment of this module only to find that when I submitted it, some of my work hadn't saved properly, and I got a 10/45. The last tests I had run were all correct, so that score couldn't have been right. Sure enough, I checked the notebook and found that a portion of my hard work was missing. I was able to find a saved checkpoint in the file tree where my work was still intact. I downloaded it, opened it in VSCode and copy/pasted my answers back into the notebook. It was painstaking and annoying, but it's done. Apart from that headache, I did enjoy the project.